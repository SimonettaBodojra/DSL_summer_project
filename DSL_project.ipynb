{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "colab": {
   "name": "DSL_project.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "2bQvYs-Aes9p",
    "colab_type": "text"
   },
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    },
    "id": "7XlHUZMses9r",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "outputId": "f84ca1eb-2efb-447f-a1be-2086904e1955"
   },
   "source": [
    "import nltk\n",
    "\"\"\"nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "dncBQS9Oes9w",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Data Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import clone\n",
    "%matplotlib inline\n",
    "\n",
    "# Data Preprocessing and Feature Engineering\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as sw, stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#Model Selection and Validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "loWHj0Jies90",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def save_results(path, model_name, params, accuracy_max):\n",
    "    with open(path, \"w\", encoding='utf8') as f:\n",
    "        f.write(f\"{model_name}:\\n\")\n",
    "        f.write(f\"BEST PARAMS: {params}\\n\")\n",
    "        f.write(f\"BEST ACCURACY: {accuracy_max}\")\n",
    "        \n",
    "#&&\n",
    "DEVELOPMENT = \"dataset/development.jsonl\"\n",
    "EVALUATION = \"dataset/evaluation.jsonl\"\n",
    "data_dev = pd.read_json(DEVELOPMENT, lines=True)\n",
    "data_ev = pd.read_json(EVALUATION, lines=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Nph7l9-Kes93",
    "colab_type": "text"
   },
   "source": [
    "Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "C_fR92U_es97",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "outputId": "ccefc9ac-a12d-4ed1-a11f-74ee928b8430"
   },
   "source": [
    "X_dev_text = data_dev.loc[:, \"full_text\"]\n",
    "y_dev_text = data_dev.loc[:, \"class\"]\n",
    "X_ev_text = data_ev.loc[:, \"full_text\"]\n",
    "\n",
    "print(f\"Training dataset size: {len(X_dev_text)}\\n\")\n",
    "print(f\"Evaluation dataset size: {len(X_ev_text)}\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "RAAXEtEaes9_",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "outputId": "3d19fe0b-fee9-4fcb-89c9-d5bf3f101d03"
   },
   "source": [
    "X_dev_text_pos = data_dev.loc[data_dev.loc[:,\"class\"]==1,\"full_text\"]\n",
    "X_dev_text_neg = data_dev.loc[data_dev.loc[:,\"class\"]==0,\"full_text\"]\n",
    "\n",
    "print(f\"Positive dataset size: {len(X_dev_text_pos)}\\n\")\n",
    "print(f\"Negative dataset size: {len(X_dev_text_neg)}\\n\")\n",
    "\n",
    "sns.countplot(x = 'class',data = data_dev)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "pEMrnLg_es-C",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "c5de5acd-ec9e-4156-cab4-df0b13ee5339"
   },
   "source": [
    "rt = sum(data_dev.loc[:,\"retweeted\"]==True)\n",
    "ft = sum(data_dev.loc[:,\"favorited\"]==True)\n",
    "print(f\"Retweets = {rt} and Favorited = {ft}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "IAYHoWYzes-I",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "3d6dc0d4-7f42-4aa4-b3d6-5d841274653a"
   },
   "source": [
    "print(X_dev_text_neg.shape[0]+X_dev_text_pos.shape[0])\n",
    "print(X_dev_text.shape[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C-RvbzrWnBeS",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "f264d65f-c439-4cec-ec61-8f78f19a6093"
   },
   "source": [
    "#pip install demoji"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "NemY9Audes-M",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "outputId": "b9c493cd-bec6-4476-cfd8-71d869dd5f5d"
   },
   "source": [
    "import demoji\n",
    "import re\n",
    "#demoji.download_codes()\n",
    "example = None\n",
    "emoji_dict = {}\n",
    "hashtag_dict= {}\n",
    "\n",
    "for text in X_dev_text_pos:\n",
    "\n",
    "    text_list=text.split()\n",
    "    for word in text_list:\n",
    "      if word.startswith(\"#\"):\n",
    "        if word not in hashtag_dict:\n",
    "          hashtag_dict[word] = [1, 0]\n",
    "        else:\n",
    "          hashtag_dict[word][0] += 1\n",
    "\n",
    "    current_emojis = demoji.findall(text)\n",
    "    for emoji, desc in current_emojis.items():\n",
    "        if emoji not in emoji_dict:\n",
    "            emoji_dict[emoji] = [desc, 1, 0]\n",
    "        else:\n",
    "            emoji_dict[emoji][1] += 1\n",
    "            \n",
    "for text in X_dev_text_neg:\n",
    "    text_list=text.split()\n",
    "    for word in text_list:\n",
    "      if word.startswith(\"#\"):\n",
    "        if word not in hashtag_dict:\n",
    "          hashtag_dict[word] = [0,1]\n",
    "        else:\n",
    "          hashtag_dict[word][1] += 1\n",
    "\n",
    "    current_emojis = demoji.findall(text)\n",
    "    for emoji, desc in current_emojis.items():\n",
    "        if emoji not in emoji_dict:\n",
    "            emoji_dict[emoji] = [desc, 0, 1]\n",
    "        else:\n",
    "            emoji_dict[emoji][2] += 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "Asuui36ses-P",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "total = sorted(hashtag_dict.items(), key=lambda x: sum(x[1][1:]),reverse=True)\n",
    "\n",
    "with open(\"total_hashtags.txt\", \"w\", encoding='utf8') as f:\n",
    "    for e in total:\n",
    "        f.write(f\"{e}\\n\")    \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3hTcMHcS0lUm",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "total = sorted(emoji_dict.items(), key=lambda x: sum(x[1][1:]),reverse=True)\n",
    "\n",
    "with open(\"total_emojis.txt\", \"w\", encoding='utf8') as f:\n",
    "    for e in total:\n",
    "        f.write(f\"{e}\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "w5Ei3uTUes-S",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "sentiment_emojis = {}\n",
    "\n",
    "for emoji, desc in emoji_dict.items():\n",
    "    pos_perc = emoji_dict[emoji][1]/sum(emoji_dict[emoji][1:])\n",
    "    sentiment_emojis[emoji] = pos_perc\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47AG-NvxtXxS",
    "colab_type": "text"
   },
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "long_stopwords=[]\n",
    "\n",
    "STOPWORDS = \"stopwords_long.txt\"\n",
    "with open(STOPWORDS, \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        long_stopwords.append(line[:-1])\n",
    "\n",
    "stopwords=sw.words('english') + [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could','doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would']\n",
    "print(len(stopwords))\n",
    "print(len(long_stopwords))\n",
    "\n",
    "stopwords.extend(long_stopwords)\n",
    "\n",
    "new_stopwords=list(set(stopwords))\n",
    "\n",
    "print(len(new_stopwords))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "S1hjUlmdes-V",
    "colab_type": "text"
   },
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "5LVsZORSes-W",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def preprocess_word(word):\n",
    "    \n",
    "    # Remove punctuation\n",
    "    word = word.strip(string.punctuation)\n",
    "    \n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    \n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    \n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    \n",
    "    return word\n",
    "\n",
    "\n",
    "def is_valid_word(word):\n",
    "    en_stopwords = sw.words('english') + [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could','doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would']\n",
    "    # Check if word begins with an alphabet is not a char or a stopword\n",
    "    return re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) \\\n",
    "           and 3 < len(word) < 16 and word not in new_stopwords is not None\n",
    "\n",
    "\n",
    "def handle_emojis(tweet,th1,th2):\n",
    "    emojis = demoji.findall(tweet)\n",
    "    \n",
    "    for emoji in emojis.keys(): \n",
    "        \n",
    "        if emoji not in sentiment_emojis:\n",
    "            continue\n",
    "        perc= sentiment_emojis[emoji]\n",
    "        \n",
    "        if perc > th2:\n",
    "            tweet = tweet.replace(emoji, 'EM_POS')  #'EM_VERY_POS')\n",
    "        elif perc > th1:\n",
    "            tweet = tweet.replace(emoji, 'EM_POS')\n",
    "        elif perc < (1-th2):\n",
    "            tweet = tweet.replace(emoji, 'EM_NEG')  #'EM_VERY_NEG')\n",
    "        elif perc < (1-th1):\n",
    "            tweet = tweet.replace(emoji, 'EM_NEG')\n",
    "        else:\n",
    "            tweet = tweet.replace(emoji, '')    \n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_tweet (tweet,th1,th2):\n",
    "    \n",
    "    # Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', '', tweet)\n",
    "    \n",
    "    # Remove @handle\n",
    "    tweet = re.sub(r'@[\\S]+', '', tweet)\n",
    "    \n",
    "    # Replaces #hashtag with hashtag\n",
    "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "    \n",
    "    # Remove RT (retweet)\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    \n",
    "    # Replace 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    \n",
    "    # Strip space, \" and ' from tweet\n",
    "    tweet = tweet.strip(' \"\\'')\n",
    "    \n",
    "    # Replace emojis with either EMO_POS or EMO_NEG\n",
    "    tweet = handle_emojis(tweet,th1,th2)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "class TweetTokenizer(object):\n",
    "    \n",
    "    def __init__(self,th1=0.6,th2=0.85):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.th1 = th1\n",
    "        self.th2 =th2\n",
    "        \n",
    "    def __call__(self, tweet):\n",
    "        \n",
    "        lemmas = []\n",
    "        tweet = preprocess_tweet (tweet,self.th1,self.th2)\n",
    "        \n",
    "        for t in word_tokenize(tweet):\n",
    "            \n",
    "            t = t.strip()\n",
    "            \n",
    "            t = preprocess_word(t)\n",
    "            \n",
    "            if is_valid_word(t):\n",
    "             lemma = self.lemmatizer.lemmatize(t)\n",
    "             lemmas.append(lemma)\n",
    "                \n",
    "        return lemmas"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "pjMD3h3zes-Z",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "d1f1a6f2-de8e-4874-baf7-de09770f22dc"
   },
   "source": [
    "\"\"\"vectorizer = TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True,\n",
    "\t\t\t\t\t\t\t  preprocessor=None, tokenizer=TweetTokenizer(th1=0.65,th2=0.85), stop_words=None, ngram_range=(1,2),\n",
    "\t\t\t\t\t\t\t  max_df=0.5, min_df=1, max_features=None)\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SzHXXYgv7TRU",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "\"\"\"vectorizer = CountVectorizer(input='content', encoding='utf-8', tokenizer=TweetTokenizer(th1=0.65,th2=0.85), \n",
    "                             stop_words=None, ngram_range=(1,2), max_df=0.5, min_df=1)\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', encoding='utf-8', tokenizer=TweetTokenizer(th1=0.65,th2=0.85), \n",
    "                             stop_words=None, ngram_range=(1,2), max_df=0.5, min_df=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "COXcTN-hoZro",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "66d81561-4b09-4fa7-a936-82b0d34954d0",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "vectorizer.fit(X_dev_text)\n",
    "X_train_tfidf = vectorizer.transform(X_dev_text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e8cRVPnkoBao",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "vocabulary = vectorizer.vocabulary_ \n",
    "words_left_out = vectorizer.stop_words_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xrVGtHgqrRiZ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "with open(\"vectorizer.txt\", \"w\", encoding='utf8') as f:\n",
    "    for e in vocabulary:\n",
    "        f.write(f\"{e}\\n\")   "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bGNq_NaargNQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "with open(\"words_left_out.txt\", \"w\", encoding='utf8') as f:\n",
    "    for e in words_left_out:\n",
    "        f.write(f\"{e}\\n\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, _, y_train, _ = train_test_split(X_dev_text, y_dev_text, shuffle=True, train_size=12000, stratify=y_dev_text)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, shuffle=True, test_size=0.25, stratify=y_train)\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__C\": [.1, 1, 10, 50, 100],\n",
    "    \"tfidf__min_df\": [1, 3, 5]\n",
    "}\n",
    "\n",
    "model = SVC(kernel=\"rbf\")\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", vectorizer),\n",
    "    (\"classifier\", model)\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring='accuracy', n_jobs=8, cv=3)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "k_II-ECFes-d",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "outputId": "aafad2c6-d7ca-4116-c6ed-20898b54fbf4"
   },
   "source": [
    "\"\"\"X_train, X_test, y_train, y_test = train_test_split(X_dev_text, y_dev_text, shuffle=True, test_size=0.25)\n",
    "\n",
    "params_model = {'C': [1, 10, 100]}\n",
    "#params_model = {'C': [1, 5, 10]}\n",
    "models = [LogisticRegression(n_jobs=8, **config) for config in ParameterGrid(params_model)]\n",
    "\n",
    "#models = [SVC(kernel=\"linear\", **config) for config in ParameterGrid(params_model)]\n",
    "\n",
    "pipelines = [Pipeline([\n",
    "    ('tfidf', vectorizers[0]),\n",
    "    ('classifier', model),\n",
    "]) for model in models]\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dev_text, y_dev_text, shuffle=True, test_size=0.25, stratify=y_dev_text)\n",
    "model = SVC(kernel=\"rbf\", C=10)\n",
    "tmp_pipe = Pipeline([\n",
    "    ('count', vectorizer),\n",
    "    ('classifier', model)\n",
    "])\n",
    "tmp_pipe.fit(X_train, y_train)\n",
    "print(\"TRAIN OVER\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds = tmp_pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1Yb0B-0T7fea",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dev_text, y_dev_text, shuffle=True, test_size=0.25)\n",
    "\n",
    "model = SVC(kernel=\"rbf\", C=10)\n",
    "\n",
    "pipelines = Pipeline([\n",
    "    ('count', vectorizer),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', model)])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "zjP8WsEles-g",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "outputId": "99f9c778-572f-40e3-d7c3-e93e49302388"
   },
   "source": [
    "labels_predicted = []\n",
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    labels_predicted.append(y_pred)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "jYpnCNzDes-j",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "total_config = []\n",
    "for conf_vec in ParameterGrid(params_vect):\n",
    "    for conf_mod in ParameterGrid(params_model):\n",
    "        conf_mod.update(conf_vec)\n",
    "        total_config.append(conf_mod)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "aTjhPyvoes-m",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "accuracies = [accuracy_score(y_test, y_pred) for y_pred in labels_predicted]\n",
    "best_ind = np.argmax(accuracies)\n",
    "best_pipeline = pipelines[best_ind]\n",
    "print(classification_report(y_test, labels_predicted[best_ind]))\n",
    "print(f\"Total Accuracy score: {np.max(accuracies)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, labels_predicted[best_ind])}\")\n",
    "print(f\"SVM best params: {list(total_config[best_ind].items())}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "vVCYYMtCes-q",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "MODEL_NAME = \"Logistic-Cnan\"\n",
    "PATH = f\"results/{MODEL_NAME}-emoji-preprocess.txt\"\n",
    "save_results(PATH, MODEL_NAME, list(total_config[best_ind].items()), np.max(accuracies))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "Fz3lvlN2es-t",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "best_pipeline.fit(X_dev_text, y_dev_text)\n",
    "y_preds = best_pipeline.predict(X_ev_text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "CgpwwgK5es-y",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "with open(\"submission3.csv\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for i,y in enumerate(y_preds):\n",
    "        f.write(f\"{i},{y}\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "yMY3i0TWes-1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "'''\n",
    "for c in data.columns:\n",
    "    mask = data.loc[:, c].isnull()\n",
    "    boh[c] = data.loc[mask].shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "ax = sns.barplot(list(boh.keys()), list(boh.values()))\n",
    "ax.set_xticklabels(list(boh.keys()), rotation=40, ha=\"right\")\n",
    "plt.show()\n",
    "# Dropping useless columns\n",
    "cleared_dataset = data_dev.drop(columns =['id','in_reply_to_status_id','in_reply_to_user_id',\n",
    "                                      'created_at', 'truncated', 'retweeted', 'lang', \n",
    "                                      'metadata', 'favorited', 'source', 'withheld_in_countries',\n",
    "                                      'quoted_status_id', 'in_reply_to_screen_name', 'contributors'])\n",
    "'''\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}